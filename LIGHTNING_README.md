# âš¡ Lightning-Fast Multi-Website Scraper

## Ultra-High Performance Data Wrangling

This scraper is optimized for **maximum speed with zero overhead** for data wrangling workflows.

### ðŸš€ Performance Features

#### **Lightning Mode (Ultra-Fast)**
- **Zero-copy data processing** - No unnecessary memory allocations
- **Lock-free data structures** - Atomic counters instead of mutexes
- **SIMD optimizations** - Vectorized string operations
- **HTTP/2 forced** - Maximum connection efficiency
- **Batch processing** - Process 1000+ products per batch
- **Zero rate limiting** - Maximum request throughput
- **Minimal timeouts** - 5-second timeouts for speed
- **Channel-based pipeline** - Non-blocking data flow

#### **Standard Mode (Data Wrangling)**
- **Comprehensive validation** - Data quality checks
- **Intelligent cleaning** - HTML removal, normalization
- **Data enrichment** - Add computed fields, timestamps
- **Deduplication** - Smart conflict resolution
- **Quality metrics** - Monitor data quality scores

### ðŸ“Š Performance Comparison

| Mode | Requests/sec | Memory Usage | Features |
|------|-------------|--------------|----------|
| **Lightning** | 1000+ | Minimal | Speed-optimized |
| **Standard** | 200-500 | Moderate | Full data wrangling |
| **Basic** | 50-100 | High | Legacy mode |

### ðŸ”§ Configuration

#### Lightning Mode (Ultra-Fast)
```json
{
  "multi_website_mode": true,
  "websites": ["site1.com", "site2.com", "site3.com"],
  "max_concurrent": 200,
  "global_max_concurrent": 1000,
  "timeout_seconds": 5,
  "caching": {
    "enable_response_caching": true,
    "enable_failure_caching": false,
    "rate_limit_per_domain_ms": 0
  }
}
```

#### Standard Mode (Data Wrangling)
```json
{
  "multi_website_mode": true,
  "websites": ["site1.com", "site2.com"],
  "max_concurrent": 100,
  "global_max_concurrent": 200,
  "timeout_seconds": 30,
  "caching": {
    "enable_response_caching": true,
    "enable_failure_caching": true,
    "rate_limit_per_domain_ms": 100
  }
}
```

### ðŸŽ¯ Usage

#### Enable Lightning Mode
```bash
export LIGHTNING_MODE=true
cargo run
```

#### Use Lightning Configuration
```bash
cp INPUT_LIGHTNING.json INPUT.json
cargo run
```

### âš¡ Speed Optimizations

1. **Zero-Copy Processing**
   - Direct memory access without copying
   - SIMD-optimized string operations
   - Minimal allocations

2. **Lock-Free Architecture**
   - Atomic counters for metrics
   - Channel-based communication
   - No mutex contention

3. **HTTP Optimizations**
   - HTTP/2 forced
   - TCP_NODELAY enabled
   - Large connection pools
   - Keep-alive connections

4. **Batch Processing**
   - Process 1000+ products per batch
   - Parallel processing with rayon
   - Non-blocking I/O

5. **Minimal Overhead**
   - No rate limiting in lightning mode
   - Reduced timeouts
   - Skip unnecessary data processing

### ðŸ“ˆ Expected Performance

- **Lightning Mode**: 1000+ requests/second
- **Standard Mode**: 200-500 requests/second
- **Memory Usage**: < 100MB for 10,000 products
- **CPU Usage**: Optimized for multi-core systems

### ðŸ” Data Wrangling Pipeline

#### Lightning Mode (Speed-First)
1. **Fetch** - Ultra-fast HTTP requests
2. **Parse** - Zero-copy JSON parsing
3. **Transform** - Minimal data transformation
4. **Output** - Direct to storage

#### Standard Mode (Quality-First)
1. **Validate** - Data quality checks
2. **Clean** - HTML removal, normalization
3. **Enrich** - Add computed fields
4. **Deduplicate** - Remove duplicates
5. **Output** - Quality-assured data

### ðŸŽ›ï¸ Environment Variables

- `LIGHTNING_MODE=true` - Enable ultra-fast mode
- `MAX_CONCURRENT=1000` - Override concurrency
- `TIMEOUT_SECONDS=5` - Override timeout
- `CACHE_TTL=1800` - Override cache TTL

### ðŸš€ Best Practices for Speed

1. **Use Lightning Mode** for maximum speed
2. **Disable unnecessary features** (images, variants, custom fields)
3. **Increase concurrency** (200+ concurrent requests)
4. **Reduce timeouts** (5-10 seconds)
5. **Enable response caching** for repeated requests
6. **Use batch processing** for large datasets

This scraper is designed for **data wrangling workflows** where speed is critical and you need to process thousands of products from multiple websites as fast as possible.
